# 开发日志 - 疲劳驾驶检测项目 TFLite 迁移与调试

本文档记录了将 YOLOv8 PyTorch 模型迁移到 TFLite 格式，并在树莓派目标设备上进行集成和调试的主要过程、遇到的问题及解决方案。

## 阶段一：模型转换与初步集成

**目标**:
*   将训练好的眼睛检测和打哈欠检测 YOLOv8 PyTorch 模型 (`.pt`) 转换为 TFLite 格式 (`.tflite`)，包括 FP16 和 INT8 量化版本。
*   更新 `适配树莓派.md` 文档，记录 TFLite 转换和集成步骤。
*   修改 `src/ai_logic.py` 以使用 TFLite Runtime 进行推理。
*   修改 `app.py` 以支持加载不同的 TFLite 模型权重，并方便测试。

**主要变更**:
1.  **文档更新 (`适配树莓派.md`)**:
    *   添加了关于 YOLOv8 模型导出为 TFLite 的说明，指出模型已转换并存放于 `model/tflite/`。
    *   详细描述了如何在 `src/ai_logic.py` 中集成 TFLite 模型，包括依赖安装、模型加载、预处理、推理执行和后处理。
    *   增加了测试不同 TFLite 模型权重（INT8, FP16,不同输入尺寸）的说明。

2.  **AI 推理逻辑修改 (`src/ai_logic.py`)**:
    *   移除了 `ultralytics` YOLO 对象的直接使用。
    *   引入 `tflite_runtime.interpreter.Interpreter` (或 `tensorflow.lite.python.interpreter.Interpreter` 作为备选) 来加载 `.tflite` 模型。
    *   在 `AIDrowsinessProcessor` 的 `__init__` 方法中初始化 TFLite解释器，获取输入输出张量的详细信息。
    *   实现了针对 TFLite 模型的图像预处理逻辑：调整大小、归一化（例如，FP16 模型通常需要 `float32` 输入，范围 `0-1`；INT8 模型可能需要 `uint8` 输入并根据其量化参数进行调整）。
    *   重写了 `_parse_tflite_output` 方法来解析 TFLite 模型的原始输出张量。YOLOv8 导出的 TFLite 模型输出格式（通常为 `[1, num_boxes, 5 + num_classes]` 或其转置形式，如 `[1, 6, 2100]` 其中 6 代表 `x,y,w,h,conf,class_id`）需要特别处理。

3.  **主应用逻辑修改 (`app.py`)**:
    *   添加了 `argparse` 模块，允许通过命令行参数指定要加载的眼睛和打哈欠 TFLite 模型路径。
    *   引入了 `MODEL_CONFIGURATIONS` 字典，预定义了多组模型配置（如 `int8_320`, `fp16_640` 等），方便在代码中通过 `DEFAULT_MODEL_CONFIG_KEY` 切换默认模型。命令行参数可以覆盖这些默认设置。
    *   修改 `initialize_resources` 函数，使其根据提供的路径加载 TFLite 模型到 `AIDrowsinessProcessor`。

4.  **依赖更新 (`requirements.txt`)**:
    *   最初尝试添加 `tensorflow-lite>=2.6.0` 以支持 TFLite Runtime。

## 阶段二：调试浏览器无画面问题

在完成初步集成后，遇到的核心问题是在浏览器中无法显示视频流（先是破损图片图标，后是持续黑屏）。

**调试过程与解决方案**:

1.  **TFLite Runtime 安装问题 (树莓派环境)**:
    *   **问题**: 在树莓派上执行 `pip install -r requirements.txt` 时，`tensorflow-lite` 包找不到匹配的分发版本 (`ERROR: No matching distribution found for tensorflow-lite`)。
    *   **诊断**: 通过网络搜索确认 `tensorflow-lite` 在 PyPI 上可能没有为所有 ARM 架构提供预编译包，而 `tflite-runtime` 是更通用的选择。
    *   **解决**:
        *   将 `requirements.txt` 中的 `tensorflow-lite>=2.6.0` 修改为 `tflite-runtime`。
        *   指导用户更新 pip (`python3 -m pip install --upgrade pip`) 并重新安装依赖。此步骤最终成功安装了 TFLite Runtime。
        *   同步更新了 `适配树莓派.md` 文档中的依赖安装说明。

2.  **视频流生成逻辑错误 (`app.py` - `video_stream_generator`)**:
    *   **问题**: 即便 TFLite Runtime 安装成功，浏览器依然黑屏。
    *   **诊断**: 开始怀疑 `video_stream_generator` 函数内部的MJPEG帧编码或传输存在问题。通过添加日志进行追踪。
    *   **发现与解决 1 (`NameError: name 'ret' is not defined`)**:
        *   日志（通过逐步添加 `print` 语句）定位到 `cv2.imencode('.jpg', ...)` 的返回值 `ret` 在被条件判断前未被赋值。原因是 `imencode` 调用位置错误。
        *   **修复**: 将 `ret, encoded_image = cv2.imencode('.jpg', current_frame_to_display)` 移至 `current_frame_to_display` 被（可能被 `draw_overlays` 修改）确定之后，且在 `if not ret:` 判断之前。

3.  **线程间数据流与状态同步问题 (`app.py`)**:
    *   **问题**: 修复 `ret` 变量问题后，依旧黑屏。终端日志显示 `[CAMERA_THREAD] CRITICAL ERROR in loop: name 'APP_STATE' is not defined`。
    *   **诊断**: `camera_thread_function` 尝试通过一个名为 `APP_STATE` 的对象访问锁 (`raw_frame_lock`) 和队列 (`frame_queue_for_ai`)，但 `APP_STATE` 对象并未在全局正确定义或初始化。实际上，这些锁和队列是直接在全局作用域定义的。
    *   **修复**:
        *   修改 `camera_thread_function`（以及后来看来也影响到 `ai_processing_thread_function` 和 `video_stream_generator` 的类似情况），使其直接使用全局定义的 `raw_frame_lock`, `frame_queue_for_ai`, `stats_lock` 等。
        *   修正了这些线程函数顶部的 `global` 声明，确保所有实际使用的全局变量（如 `stop_event`, `APP_ARGS` 以及直接使用的锁和队列名）都被正确声明。

4.  **TFLite 模型输出解析与 AI 逻辑 (`src/ai_logic.py`)**:
    *   **问题**: 修复 `APP_STATE` 问题后，摄像头线程和 AI 线程开始打印日志，表明它们在运行，但浏览器依然黑屏，且 AI 模型似乎未正确检测目标。
    *   **诊断**: 在 `src/ai_logic.py` 的 `_parse_tflite_output` 函数中加入了详细的调试打印，以观察模型原始输出和解析过程。
        *   日志显示模型输出的 `shape` 为 `(1, 6, 2100)`。代码中存在一个基于输出第二维和第三维大小关系的转置逻辑，将其转换为 `(1, 2100, 6)`，这符合 YOLO 输出的常见格式（NMS 可能已在模型内部或导出时集成）。
        *   初步测试 INT8 模型时，注意到其输入 `dtype` 和 `quantization` 参数，确保预处理符合要求（FP16 模型使用 `float32` 输入，INT8 模型通常使用 `uint8`）。大部分调试后续集中在使用 FP16 模型上。
        *   日志显示 `_parse_tflite_output` 大部分时间返回 `BestClassID: -1, MaxConfidence: -1.0`，表明未达到置信度阈值（0.3）或无检测。但偶尔会有成功检测的记录，说明解析逻辑通路基本正确。

5.  **`video_stream_generator` 循环执行问题 (`app.py`)**:
    *   **问题**: 尽管摄像头线程捕获帧，AI 线程处理帧，但浏览器依然黑屏。日志分析显示，`video_stream_generator` 函数打印了初始的 "Video stream generator started." 日志后，在检测期间，其内部 `while True:` 循环中添加的详细日志（如获取帧、编码、yield）完全没有出现。仅在停止检测时，才打印退出循环的日志。
    *   **诊断**: 这表明 `video_stream_generator` 在进入其主工作循环或循环的早期阶段就可能阻塞或意外退出了，导致没有帧被 `yield` 给客户端。
    *   **尝试解决**:
        *   在 `video_stream_generator` 函数的更早位置添加日志：函数被调用时、`while True:` 循环初始化参数后、进入 `while True:` 循环的顶部、进入 `try:` 块的顶部。目的是精确定位执行流在何处中断。
        *   使对 `APP_ARGS.display_fps` 的访问更稳健，防止因其未定义或为 `None` 导致错误。

6.  **日志记录持久化**:
    *   **需求**: 为了方便分析和避免因控制台关闭导致日志丢失，需要将所有控制台输出保存到文件。
    *   **实现**:
        *   在 `app.py` 中添加了 `sys`, `os`, `datetime` 导入。
        *   定义了一个 `Tee` 类，用于将输出流（如 `sys.stdout`, `sys.stderr`）同时重定向到原始流和指定文件。
        *   定义了 `setup_file_logging` 函数，用于创建 `logs` 目录（如果不存在），生成带时间戳的日志文件名，并将 `sys.stdout` 和 `sys.stderr` 重定向到 `Tee` 的实例。
        *   在 `if __name__ == '__main__':` 块的开头调用 `setup_file_logging()`。
    *   **结果**: 成功实现了日志输出到 `logs/app_YYYYMMDD_HHMMSS.log` 文件，同时保留控制台输出。

## 当前状态 (截至记录此日志时)

*   应用能够启动，摄像头线程捕获帧，AI 线程处理帧。
*   TFLite 模型（特别是 FP16 版本）能够加载并进行推理，偶尔能检测到目标。
*   **核心问题依然是浏览器端没有视频画面。**
*   通过在 `app.py` 的 `video_stream_generator` 中添加 `flush=True` 到 `print` 语句，确认了当检测激活时，生成器内部的循环（帧计数增加）确实在执行，但日志显示 `yield` 操作可能存在问题或客户端未正确接收。

## 阶段三：通过简化调试程序定位并解决视频流问题

**调试过程与解决方案**: 

1.  **创建简化视频流测试程序 (`debug_video_stream.py`)**:
    *   **动机**: 为了排除主应用中复杂逻辑（AI处理、SocketIO、多状态同步）的干扰，创建一个最小化的 Flask 应用，仅包含摄像头捕获和 MJPEG 视频流生成功能。
    *   **实现**: `debug_video_stream.py` 包含一个简单的摄像头线程，将帧传递给视频流生成器，后者直接编码并 `yield` 帧。所有 `print` 语句使用 `flush=True`。
    *   **初步测试 (`debug_video_stream.py`) 失败**: 首次运行简化程序，浏览器依然没有画面。日志显示 `video_stream_generator_debug` 被调用，但其内部循环的日志（如帧编码、yield前后）没有出现，与主应用现象类似。

2.  **深入分析 `debug_video_stream.py` 的日志**:
    *   在 `debug_video_stream.py` 的 `video_stream_generator_debug` 中添加更详细的日志，包括编码成功、`PRE-YIELD` 和 `POST-YIELD`。
    *   **突破性发现**: 运行带有详细日志的 `debug_video_stream.py` 后，浏览器成功显示了视频流！控制台日志清晰地显示了每一帧从编码到 `PRE-YIELD` 再到 `POST-YIELD` 的完整过程。
    *   这证明了 Flask + OpenCV MJPEG 流在基本层面上是可以在该环境工作的。

3.  **对比 `debug_video_stream.py` 与 `app.py` 的关键差异**:
    *   **占位符与初始帧逻辑**: 
        *   `debug_video_stream.py` 中的 `video_stream_generator_debug`：在其循环的每次迭代中，如果无法获取到真实摄像头帧 (`latest_raw_frame is None`)，它会主动创建一个"Waiting for camera..."的占位符图像，并**继续编码和 `yield` 这个占位符图像**。
        *   `app.py` 中的 `video_stream_generator` (问题版本)：当 `latest_raw_frame is None` 时，其行为取决于 `detection_active` 状态：
            *   如果 `detection_active` 为 `False` (检测未开始或已停止)，它会尝试 `yield` 一个"Detection Stopped or Starting..."的占位符图像。
            *   **关键问题点**：如果 `detection_active` 为 `True` (检测已启动，但摄像头线程可能尚未提供第一帧给 `latest_raw_frame`)，代码会执行 `time.sleep(delay_per_frame / 2)` 然后 `continue`，**从而完全跳过了当前迭代的 `yield` 操作**。

4.  **问题根源定位**:
    *   当用户点击"Start Detection"后，`detection_active` 变为 `True`。此时 `video_stream_generator` 重新开始（或继续）其循环。如果此时 `camera_thread_function` 尚未捕获并填充 `latest_raw_frame`（可能存在微小的启动延迟或线程调度间隙），`video_stream_generator` 会因为 `latest_raw_frame is None` 且 `detection_active is True` 而进入 `continue` 分支。 
    *   这意味着服务器的生成器没有立即 `yield` 任何数据给已经连接并等待 MJPEG 流的浏览器。这种延迟或"沉默"可能导致浏览器认为连接超时、挂起或流已结束，因此即使后续 `latest_raw_frame` 有了数据，浏览器也可能不再正确处理后续的帧。

5.  **解决方案**: 
    *   修改 `app.py` 中的 `video_stream_generator` 函数。确保在其主循环的每次迭代中，如果因为任何原因（包括检测刚启动，真实帧尚未就绪）无法获得有效的 `current_frame_to_display`，都应该 `yield` 一个合适的占位符图像，而不是简单地 `continue`。
    *   这样可以保证一旦 `/video_feed` 路由被访问，生成器会持续地向客户端发送数据帧（无论是真实图像还是占位符），维持 MJPEG 流的连续性。

**状态更新 (解决视频流问题后)**:
*   应用能够启动，摄像头线程捕获帧，AI 线程处理帧。
*   TFLite 模型能够加载并进行推理。
*   **浏览器端已能成功显示视频画面和AI叠加效果。**
*   通过 `debug_video_stream.py` 的对比测试，确认了问题在于 `app.py` 初始化视频流时，在特定条件下未及时 `yield` 帧导致。修改后，确保了流的连续性。

## 阶段四：分析 INT8 模型性能瓶颈

在解决了视频流显示问题，并且前端状态能够正常更新后，我们开始评估和优化 AI 处理性能。最初的 FP16 模型处理时间较长（约 1 秒或更多）。我们尝试切换到 `int8_320` 配置，期望通过 INT8 量化获得显著的速度提升。

**问题描述**:
切换到 `DEFAULT_MODEL_CONFIG_KEY = "int8_320"` 配置后，AI 处理总时间（`ai_frame_process_time_ms`）依然在 1000ms 左右，与 FP16 模型相比没有明显改善。

**调试与分析 (基于日志 `app_20250510_001929.log`)**:

1.  **模型加载细节确认**:
    *   日志显示 `AIDrowsinessProcessor` 初始化时，加载的眼部和打哈欠 TFLite 模型文件名确实包含 `int8`。
    *   **关键发现**: TFLite 解释器报告的输入细节如下：
        *   `Eye TFLite Input Details: dtype=<class 'numpy.float32'>, quantization=(0.0, 0)`
        *   `Yawn TFLite Input Details: dtype=<class 'numpy.float32'>, quantization=(0.0, 0)`
    *   这表明，尽管模型内部权重可能是 INT8 量化的，但其 **输入接口仍然期望 `float32` 类型的数据**，并且没有提供有效的量化参数 (scale, zero_point)。真正的全整型量化模型，其输入/输出张量本身也应该是 `int8` 或 `uint8` 类型。

2.  **性能剖析日志分析 (`[PERF_PROFILE]` 和 `[PERF_PROFILE_DETAIL]`)**:
    *   `process_frame` 函数总耗时约 1100+ ms。
    *   MediaPipe 网格处理（约 25ms）和 ROI 提取（约 1ms）非常快。
    *   **瓶颈定位**:
        *   `Both Eye state predictions took: ~720 ms` (每次调用 `_predict_eye_state_tflite` 约 360ms)
        *   `Yawn state prediction took: ~360 ms` (调用 `_predict_yawn_tflite`)
    *   进一步细化到 TFLite `invoke()` 调用耗时：
        *   `_predict_eye_state_tflite: invoke took: ~270 ms`
        *   `_predict_yawn_tflite: invoke took: ~280 ms`
    *   这些 `invoke` 调用占据了各自预测函数的大部分时间，并且是整个 `process_frame` 耗时的主要来源。

**结论**:
*   当前使用的所谓 "INT8" TFLite 模型，由于其输入接口仍为 `float32`，并未实现全整型量化。这意味着在模型推理前，输入数据（来自摄像头的 `uint8` 图像帧）在 `_preprocess_image_for_tflite` 中被转换为 `float32` (除以 255.0)。TFLite 运行时可能需要将这些 `float32` 输入再反量化回 INT8 以匹配内部权重，或者在没有专用硬件加速的情况下，部分运算可能仍在浮点模式下进行。这两种情况都会抵消 INT8 量化本应带来的大部分性能优势。
*   在树莓派这类设备上，对于期望的实时性能，单次模型 `invoke` 耗时 250-300ms 过高。

**后续行动**:
*   **核心任务**: 必须重新审视和修正 TFLite 模型的导出过程。目标是生成 **全整型量化 (full integer quantization)** 的模型，其输入和输出张量类型应为 `int8` 或 `uint8`，并包含正确的量化参数。
*   这将涉及到在模型转换时使用合适的工具和参数，特别是要提供一个有效的**代表性校准数据集 (representative dataset)**。
*   一旦获得真正的全整型量化模型，需要相应调整 `src/ai_logic.py` 中的 `_preprocess_image_for_tflite` 函数，以匹配新的整型输入要求（例如，不再进行 `/255.0` 的操作，并确保 `dtype` 正确）。

**深入分析模型导出过程 (基于 `export_tflite3.log` 和 `check_tflite_model_inputs.py` 的输出)**:

经过对用户提供的 `export_tflite.py` 脚本生成的日志 (`export_tflite3.log`) 以及模型检查脚本 (`check_tflite_model_inputs.py`) 的输出进行详细分析后，我们获得了以下关键信息：

1.  **模型输入类型确认**: `check_tflite_model_inputs.py` 的输出明确显示，当前 `/model/tflite/` 目录下所有的 `.tflite` 文件（包括文件名中带有 `_int8` 的模型），其输入张量的 `dtype` 均为 `<class 'numpy.float32'>`，且 `Quantization (scale, zero_point)` 参数为 `(0.0, 0)`。这最终证实了之前的推断：**导出的 INT8 模型输入接口仍为浮点型，并非全整型量化模型。**

2.  **模型导出日志分析 (`export_tflite3.log`)**:
    *   **环境**: 导出过程使用了 Ultralytics `8.3.129`, TensorFlow `2.19.0`, `onnx2tf 1.27.2` 等。导出在 CPU 环境下进行。
    *   **校准数据扫描疑点**:
        *   日志中出现 `Scanning ... 0 images, 100 backgrounds, 0 corrupt` 的记录。这表明虽然找到了100个文件作为校准数据，但它们可能未被 Ultralytics 的导出流程完全识别为有效的、带标签的图像，或被错误地归类。
        *   伴随出现 `WARNING ⚠️ No labels found in ...cache` 的警告。虽然INT8校准主要需要图像而非标签，但这可能暗示了数据加载器对数据集结构的某种预期未被满足。
    *   **`onnx2tf` 工具链日志**:
        *   在INT8转换部分，`onnx2tf` 的日志显示了 `fully_quantize: 0`。
        *   日志中还记录了 `input_inference_type` 和 `output_inference_type` 从 `FLOAT32` 尝试变为 `INT8`，但似乎这个设定在最终的 TFLite 模型生成中并未生效，或者由于某些条件不满足（如部分操作不支持纯整型、量化参数计算不充分等）而导致转换器回退 (fallback) 到了浮点输入/输出接口，即使内部权重可能已量化为INT8。
        *   日志中还记录了 `input_inference_type` 和 `output_inference_type` 从 `FLOAT32` 尝试变为 `INT8`，但似乎这个设定在最终的 TFLite 模型生成中并未生效，或者由于某些条件不满足（如部分操作不支持纯整型、量化参数计算不充分等）而导致转换器回退 (fallback) 到了浮点输入/输出接口，即使内部权重可能已量化为INT8。
    *   **模型大小**: 生成的 `_int8.tflite` 文件确实比 `_fp16.tflite` 文件小，表明权重可能确实被量化了，但接口问题依然存在。

**核心结论更新**:
问题的根源几乎可以肯定是**校准数据未能被 Ultralytics 的导出工具链正确和充分地用于全整型量化过程**。尽管校准文件路径配置正确，但 Ultralytics 的数据加载器可能对其内部结构（例如，是否需要特定的子目录如 `images/` 和 `labels/`）有隐式要求，或者对无标签数据的处理方式导致校准不充分，进而使得 `onnx2tf` 无法完成严格的全整型量化（即输入输出张量也为 INT8/UINT8），最终回退到生成一个仅内部权重为 INT8、但输入输出仍为 FLOAT32 的模型。

**下一步具体行动**:
*   调整校准数据集的目录结构，使其更符合 Ultralytics 可能期望的格式。
*   相应修改校准配置文件 (`eye_calibration_config.yaml`, `yawn_calibration_config.yaml`) 以反映新的目录结构。
*   重新运行模型导出脚本。
*   再次使用 `check_tflite_model_inputs.py` 检查新生成模型的输入张量类型，目标是看到 `dtype` 为 `numpy.int8` 或 `numpy.uint8`，并且 `quantization` 参数具有非零的 scale 和 zero_point 值。

## 阶段五：持续调试 INT8 全整型量化问题 - 校准数据路径与结构

在确认了当前 INT8 模型输入仍为 `float32` 后，我们将焦点放在了校准数据的提供方式上。我们进行了多轮尝试，核心目标是让 Ultralytics 的导出脚本能够正确识别并使用校准图像，以实现真正的全整型量化。

**主要尝试与发现 (基于 `export_tflite4.log` 至 `export_tflite10.log`)**:

1.  **调整校准 YAML 文件 (`eye_calibration_config.yaml`, `yawn_calibration_config.yaml`)**:
    *   **相对路径尝试**: 最初，我们将 YAML 文件中的 `path` 字段从 `../data` 修改为 `data` (或 `./data`)，假设 YAML 文件位于项目根目录，并且校准图片位于 `PROJECT_ROOT/data/calibration_images_xxx/images/`。
        *   **结果**: 导出日志依然显示找不到校准图像或将其视为背景 (例如 `ERROR ... Dataset 'eye_calibration_config.yaml' images not found...` 或 `Scanning ... 0 images, 100 backgrounds...`)。
    *   **绝对路径尝试**: 将 YAML 文件中的 `path` 字段修改为指向 `PROJECT_ROOT/data/` 的绝对路径。
        *   **结果**: 路径识别似乎正确了，日志显示开始扫描 `PROJECT_ROOT/data/calibration_images_eye/images` 等。但依然报出 `WARNING ⚠️ No labels found in ...cache` 和 `Scanning ... 0 images, 100 backgrounds...`。
    *   **调整 `train` 和 `val` 指向父目录**: 修改 YAML，使 `path` 指向 `PROJECT_ROOT/data/` (绝对路径)，并将 `train` 和 `val` 指向 `calibration_images_eye` (而不是 `calibration_images_eye/images`)，期望 Ultralytics 自动寻找 `images` 和 `labels` 子目录。
        *   **结果**: 依然是 `0 images, 100 backgrounds`。
    *   **注释 `path`, `train`/`val` 使用绝对路径**: 注释掉 YAML 中的 `path` 字段，直接为 `train` 和 `val` 提供指向 `PROJECT_ROOT/data/calibration_images_xxx/images/` 的绝对路径。
        *   **结果**: 没有改善，依旧是 `0 images, 100 backgrounds`。
    *   **遵循网络搜索结果的相对路径**: 根据 Ultralytics GitHub issue ([#14121](https://github.com/ultralytics/ultralytics/issues/14121)) 的启示，如果 `path` 字段被注释掉或设为 `.`，则 `train` 和 `val` 路径应相对于 YAML 文件本身。我们将 `train` 和 `val` 设置为 `data/calibration_images_eye/images`。
        *   **结果 (`export_tflite10.log`)**: 依然是 `0 images, 100 backgrounds`。

2.  **校准数据目录结构调整**:
    *   **创建空的 `labels` 目录**: 根据 Ultralytics 可能需要 `images` 和 `labels` 并列子目录的猜测，我们为 `data/calibration_images_eye/` 和 `data/calibration_images_yawn/` 创建了空的 `labels/` 子目录。
        *   **结果**: 未能解决 `0 images, 100 backgrounds` 的问题。

3.  **校准图像转移脚本 (`scripts/Transfer_Calibration_Images.py`) 问题**:
    *   **问题识别**: 在反复调整 YAML 配置无效后，我们怀疑校准图像本身并未按预期放置到 `PROJECT_ROOT/data/calibration_images_xxx/images/` 目录中。
    *   **脚本分析**: 分析 `Transfer_Calibration_Images.py` 脚本发现，其目标输出目录最初被设置为 `data/calibration_images_xxx/` 而不是 `data/calibration_images_xxx/images/`。
    *   **修复尝试**: 多次尝试使用 `edit_file` 工具修改脚本，将图片复制到正确的 `images` 子目录。然而，由于 `edit_file` 工具的行为与预期不符（例如，有时会错误地截断文件或未能应用更改），导致脚本修复过程一波三折。
    *   **最终状态**: 尽管多次尝试修正并确认脚本中的路径变量理论上是正确的 (指向 `.../images/`)，但实际运行脚本后的输出日志（例如，打印的最终复制路径）和文件系统检查，反复显示图片仍被复制到了父目录 (`data/calibration_images_xxx/`)。这表明实际执行的脚本版本与我们编辑后的版本存在持续的不一致。

**总结与当前困境**:

尽管对校准配置文件和校准数据的物理存放路径进行了大量尝试和调整，截至 `export_tflite10.log`，Ultralytics 的导出流程始终未能将提供的校准图片识别为有效数据进行 INT8 量化。错误日志持续指向 "0 images, 100 backgrounds"，表明模型转换时未能找到或正确使用校准集，因此生成的 INT8 TFLite 模型输入输出接口依然是 `float32`。

在最近的交互中，`scripts/Transfer_Calibration_Images.py` 文件已被删除，这可能预示着需要一种全新的、更可靠的方式来准备和提供校准数据集。

## 后续步骤推测
*   分析 `video_stream_generator` 最新的、更详细的日志，找出其未按预期循环和 `yield` 帧的原因。
*   检查是否有任何未捕获的异常或阻塞点在 `video_stream_generator` 的早期逻辑中。
*   确认从 `latest_raw_frame` 获取图像数据是否可靠且持续。
*   ~~一旦视频流问题解决，将进一步测试不同 TFLite 模型的性能和准确性。~~ (此项已可进行)
