# 开发日志 - 疲劳驾驶检测项目 TFLite 迁移与调试

本文档记录了将 YOLOv8 PyTorch 模型迁移到 TFLite 格式，并在树莓派目标设备上进行集成和调试的主要过程、遇到的问题及解决方案。

## 阶段一：模型转换与初步集成

**目标**:
*   将训练好的眼睛检测和打哈欠检测 YOLOv8 PyTorch 模型 (`.pt`) 转换为 TFLite 格式 (`.tflite`)，包括 FP16 和 INT8 量化版本。
*   更新 `适配树莓派.md` 文档，记录 TFLite 转换和集成步骤。
*   修改 `src/ai_logic.py` 以使用 TFLite Runtime 进行推理。
*   修改 `app.py` 以支持加载不同的 TFLite 模型权重，并方便测试。

**主要变更**:
1.  **文档更新 (`适配树莓派.md`)**:
    *   添加了关于 YOLOv8 模型导出为 TFLite 的说明，指出模型已转换并存放于 `model/tflite/`。
    *   详细描述了如何在 `src/ai_logic.py` 中集成 TFLite 模型，包括依赖安装、模型加载、预处理、推理执行和后处理。
    *   增加了测试不同 TFLite 模型权重（INT8, FP16,不同输入尺寸）的说明。

2.  **AI 推理逻辑修改 (`src/ai_logic.py`)**:
    *   移除了 `ultralytics` YOLO 对象的直接使用。
    *   引入 `tflite_runtime.interpreter.Interpreter` (或 `tensorflow.lite.python.interpreter.Interpreter` 作为备选) 来加载 `.tflite` 模型。
    *   在 `AIDrowsinessProcessor` 的 `__init__` 方法中初始化 TFLite解释器，获取输入输出张量的详细信息。
    *   实现了针对 TFLite 模型的图像预处理逻辑：调整大小、归一化（例如，FP16 模型通常需要 `float32` 输入，范围 `0-1`；INT8 模型可能需要 `uint8` 输入并根据其量化参数进行调整）。
    *   重写了 `_parse_tflite_output` 方法来解析 TFLite 模型的原始输出张量。YOLOv8 导出的 TFLite 模型输出格式（通常为 `[1, num_boxes, 5 + num_classes]` 或其转置形式，如 `[1, 6, 2100]` 其中 6 代表 `x,y,w,h,conf,class_id`）需要特别处理。

3.  **主应用逻辑修改 (`app.py`)**:
    *   添加了 `argparse` 模块，允许通过命令行参数指定要加载的眼睛和打哈欠 TFLite 模型路径。
    *   引入了 `MODEL_CONFIGURATIONS` 字典，预定义了多组模型配置（如 `int8_320`, `fp16_640` 等），方便在代码中通过 `DEFAULT_MODEL_CONFIG_KEY` 切换默认模型。命令行参数可以覆盖这些默认设置。
    *   修改 `initialize_resources` 函数，使其根据提供的路径加载 TFLite 模型到 `AIDrowsinessProcessor`。

4.  **依赖更新 (`requirements.txt`)**:
    *   最初尝试添加 `tensorflow-lite>=2.6.0` 以支持 TFLite Runtime。

## 阶段二：调试浏览器无画面问题

在完成初步集成后，遇到的核心问题是在浏览器中无法显示视频流（先是破损图片图标，后是持续黑屏）。

**调试过程与解决方案**:

1.  **TFLite Runtime 安装问题 (树莓派环境)**:
    *   **问题**: 在树莓派上执行 `pip install -r requirements.txt` 时，`tensorflow-lite` 包找不到匹配的分发版本 (`ERROR: No matching distribution found for tensorflow-lite`)。
    *   **诊断**: 通过网络搜索确认 `tensorflow-lite` 在 PyPI 上可能没有为所有 ARM 架构提供预编译包，而 `tflite-runtime` 是更通用的选择。
    *   **解决**:
        *   将 `requirements.txt` 中的 `tensorflow-lite>=2.6.0` 修改为 `tflite-runtime`。
        *   指导用户更新 pip (`python3 -m pip install --upgrade pip`) 并重新安装依赖。此步骤最终成功安装了 TFLite Runtime。
        *   同步更新了 `适配树莓派.md` 文档中的依赖安装说明。

2.  **视频流生成逻辑错误 (`app.py` - `video_stream_generator`)**:
    *   **问题**: 即便 TFLite Runtime 安装成功，浏览器依然黑屏。
    *   **诊断**: 开始怀疑 `video_stream_generator` 函数内部的MJPEG帧编码或传输存在问题。通过添加日志进行追踪。
    *   **发现与解决 1 (`NameError: name 'ret' is not defined`)**:
        *   日志（通过逐步添加 `print` 语句）定位到 `cv2.imencode('.jpg', ...)` 的返回值 `ret` 在被条件判断前未被赋值。原因是 `imencode` 调用位置错误。
        *   **修复**: 将 `ret, encoded_image = cv2.imencode('.jpg', current_frame_to_display)` 移至 `current_frame_to_display` 被（可能被 `draw_overlays` 修改）确定之后，且在 `if not ret:` 判断之前。

3.  **线程间数据流与状态同步问题 (`app.py`)**:
    *   **问题**: 修复 `ret` 变量问题后，依旧黑屏。终端日志显示 `[CAMERA_THREAD] CRITICAL ERROR in loop: name 'APP_STATE' is not defined`。
    *   **诊断**: `camera_thread_function` 尝试通过一个名为 `APP_STATE` 的对象访问锁 (`raw_frame_lock`) 和队列 (`frame_queue_for_ai`)，但 `APP_STATE` 对象并未在全局正确定义或初始化。实际上，这些锁和队列是直接在全局作用域定义的。
    *   **修复**:
        *   修改 `camera_thread_function`（以及后来看来也影响到 `ai_processing_thread_function` 和 `video_stream_generator` 的类似情况），使其直接使用全局定义的 `raw_frame_lock`, `frame_queue_for_ai`, `stats_lock` 等。
        *   修正了这些线程函数顶部的 `global` 声明，确保所有实际使用的全局变量（如 `stop_event`, `APP_ARGS` 以及直接使用的锁和队列名）都被正确声明。

4.  **TFLite 模型输出解析与 AI 逻辑 (`src/ai_logic.py`)**:
    *   **问题**: 修复 `APP_STATE` 问题后，摄像头线程和 AI 线程开始打印日志，表明它们在运行，但浏览器依然黑屏，且 AI 模型似乎未正确检测目标。
    *   **诊断**: 在 `src/ai_logic.py` 的 `_parse_tflite_output` 函数中加入了详细的调试打印，以观察模型原始输出和解析过程。
        *   日志显示模型输出的 `shape` 为 `(1, 6, 2100)`。代码中存在一个基于输出第二维和第三维大小关系的转置逻辑，将其转换为 `(1, 2100, 6)`，这符合 YOLO 输出的常见格式（NMS 可能已在模型内部或导出时集成）。
        *   初步测试 INT8 模型时，注意到其输入 `dtype` 和 `quantization` 参数，确保预处理符合要求（FP16 模型使用 `float32` 输入，INT8 模型通常使用 `uint8`）。大部分调试后续集中在使用 FP16 模型上。
        *   日志显示 `_parse_tflite_output` 大部分时间返回 `BestClassID: -1, MaxConfidence: -1.0`，表明未达到置信度阈值（0.3）或无检测。但偶尔会有成功检测的记录，说明解析逻辑通路基本正确。

5.  **`video_stream_generator` 循环执行问题 (`app.py`)**:
    *   **问题**: 尽管摄像头线程捕获帧，AI 线程处理帧，但浏览器依然黑屏。日志分析显示，`video_stream_generator` 函数打印了初始的 "Video stream generator started." 日志后，在检测期间，其内部 `while True:` 循环中添加的详细日志（如获取帧、编码、yield）完全没有出现。仅在停止检测时，才打印退出循环的日志。
    *   **诊断**: 这表明 `video_stream_generator` 在进入其主工作循环或循环的早期阶段就可能阻塞或意外退出了，导致没有帧被 `yield` 给客户端。
    *   **尝试解决**:
        *   在 `video_stream_generator` 函数的更早位置添加日志：函数被调用时、`while True:` 循环初始化参数后、进入 `while True:` 循环的顶部、进入 `try:` 块的顶部。目的是精确定位执行流在何处中断。
        *   使对 `APP_ARGS.display_fps` 的访问更稳健，防止因其未定义或为 `None` 导致错误。

6.  **日志记录持久化**:
    *   **需求**: 为了方便分析和避免因控制台关闭导致日志丢失，需要将所有控制台输出保存到文件。
    *   **实现**:
        *   在 `app.py` 中添加了 `sys`, `os`, `datetime` 导入。
        *   定义了一个 `Tee` 类，用于将输出流（如 `sys.stdout`, `sys.stderr`）同时重定向到原始流和指定文件。
        *   定义了 `setup_file_logging` 函数，用于创建 `logs` 目录（如果不存在），生成带时间戳的日志文件名，并将 `sys.stdout` 和 `sys.stderr` 重定向到 `Tee` 的实例。
        *   在 `if __name__ == '__main__':` 块的开头调用 `setup_file_logging()`。
    *   **结果**: 成功实现了日志输出到 `logs/app_YYYYMMDD_HHMMSS.log` 文件，同时保留控制台输出。

## 当前状态 (截至记录此日志时)

*   应用能够启动，摄像头线程捕获帧，AI 线程处理帧。
*   TFLite 模型（特别是 FP16 版本）能够加载并进行推理，偶尔能检测到目标。
*   **核心问题依然是浏览器端没有视频画面。**
*   通过在 `app.py` 的 `video_stream_generator` 中添加 `flush=True` 到 `print` 语句，确认了当检测激活时，生成器内部的循环（帧计数增加）确实在执行，但日志显示 `yield` 操作可能存在问题或客户端未正确接收。

## 阶段三：通过简化调试程序定位并解决视频流问题

**调试过程与解决方案**: 

1.  **创建简化视频流测试程序 (`debug_video_stream.py`)**:
    *   **动机**: 为了排除主应用中复杂逻辑（AI处理、SocketIO、多状态同步）的干扰，创建一个最小化的 Flask 应用，仅包含摄像头捕获和 MJPEG 视频流生成功能。
    *   **实现**: `debug_video_stream.py` 包含一个简单的摄像头线程，将帧传递给视频流生成器，后者直接编码并 `yield` 帧。所有 `print` 语句使用 `flush=True`。
    *   **初步测试 (`debug_video_stream.py`) 失败**: 首次运行简化程序，浏览器依然没有画面。日志显示 `video_stream_generator_debug` 被调用，但其内部循环的日志（如帧编码、yield前后）没有出现，与主应用现象类似。

2.  **深入分析 `debug_video_stream.py` 的日志**:
    *   在 `debug_video_stream.py` 的 `video_stream_generator_debug` 中添加更详细的日志，包括编码成功、`PRE-YIELD` 和 `POST-YIELD`。
    *   **突破性发现**: 运行带有详细日志的 `debug_video_stream.py` 后，浏览器成功显示了视频流！控制台日志清晰地显示了每一帧从编码到 `PRE-YIELD` 再到 `POST-YIELD` 的完整过程。
    *   这证明了 Flask + OpenCV MJPEG 流在基本层面上是可以在该环境工作的。

3.  **对比 `debug_video_stream.py` 与 `app.py` 的关键差异**:
    *   **占位符与初始帧逻辑**: 
        *   `debug_video_stream.py` 中的 `video_stream_generator_debug`：在其循环的每次迭代中，如果无法获取到真实摄像头帧 (`latest_raw_frame is None`)，它会主动创建一个"Waiting for camera..."的占位符图像，并**继续编码和 `yield` 这个占位符图像**。
        *   `app.py` 中的 `video_stream_generator` (问题版本)：当 `latest_raw_frame is None` 时，其行为取决于 `detection_active` 状态：
            *   如果 `detection_active` 为 `False` (检测未开始或已停止)，它会尝试 `yield` 一个"Detection Stopped or Starting..."的占位符图像。
            *   **关键问题点**：如果 `detection_active` 为 `True` (检测已启动，但摄像头线程可能尚未提供第一帧给 `latest_raw_frame`)，代码会执行 `time.sleep(delay_per_frame / 2)` 然后 `continue`，**从而完全跳过了当前迭代的 `yield` 操作**。

4.  **问题根源定位**:
    *   当用户点击"Start Detection"后，`detection_active` 变为 `True`。此时 `video_stream_generator` 重新开始（或继续）其循环。如果此时 `camera_thread_function` 尚未捕获并填充 `latest_raw_frame`（可能存在微小的启动延迟或线程调度间隙），`video_stream_generator` 会因为 `latest_raw_frame is None` 且 `detection_active is True` 而进入 `continue` 分支。 
    *   这意味着服务器的生成器没有立即 `yield` 任何数据给已经连接并等待 MJPEG 流的浏览器。这种延迟或"沉默"可能导致浏览器认为连接超时、挂起或流已结束，因此即使后续 `latest_raw_frame` 有了数据，浏览器也可能不再正确处理后续的帧。

5.  **解决方案**: 
    *   修改 `app.py` 中的 `video_stream_generator` 函数。确保在其主循环的每次迭代中，如果因为任何原因（包括检测刚启动，真实帧尚未就绪）无法获得有效的 `current_frame_to_display`，都应该 `yield` 一个合适的占位符图像，而不是简单地 `continue`。
    *   这样可以保证一旦 `/video_feed` 路由被访问，生成器会持续地向客户端发送数据帧（无论是真实图像还是占位符），维持 MJPEG 流的连续性。

**状态更新 (解决视频流问题后)**:
*   应用能够启动，摄像头线程捕获帧，AI 线程处理帧。
*   TFLite 模型能够加载并进行推理。
*   **浏览器端已能成功显示视频画面和AI叠加效果。**
*   通过 `debug_video_stream.py` 的对比测试，确认了问题在于 `app.py` 初始化视频流时，在特定条件下未及时 `yield` 帧导致。修改后，确保了流的连续性。

## 阶段四：分析 INT8 模型性能瓶颈

在解决了视频流显示问题，并且前端状态能够正常更新后，我们开始评估和优化 AI 处理性能。最初的 FP16 模型处理时间较长（约 1 秒或更多）。我们尝试切换到 `int8_320` 配置，期望通过 INT8 量化获得显著的速度提升。

**问题描述**:
切换到 `DEFAULT_MODEL_CONFIG_KEY = "int8_320"` 配置后，AI 处理总时间（`ai_frame_process_time_ms`）依然在 1000ms 左右，与 FP16 模型相比没有明显改善。

**调试与分析 (基于日志 `app_20250510_001929.log`)**:

1.  **模型加载细节确认**:
    *   日志显示 `AIDrowsinessProcessor` 初始化时，加载的眼部和打哈欠 TFLite 模型文件名确实包含 `int8`。
    *   **关键发现**: TFLite 解释器报告的输入细节如下：
        *   `Eye TFLite Input Details: dtype=<class 'numpy.float32'>, quantization=(0.0, 0)`
        *   `Yawn TFLite Input Details: dtype=<class 'numpy.float32'>, quantization=(0.0, 0)`
    *   这表明，尽管模型内部权重可能是 INT8 量化的，但其 **输入接口仍然期望 `float32` 类型的数据**，并且没有提供有效的量化参数 (scale, zero_point)。真正的全整型量化模型，其输入/输出张量本身也应该是 `int8` 或 `uint8` 类型。

2.  **性能剖析日志分析 (`[PERF_PROFILE]` 和 `[PERF_PROFILE_DETAIL]`)**:
    *   `process_frame` 函数总耗时约 1100+ ms。
    *   MediaPipe 网格处理（约 25ms）和 ROI 提取（约 1ms）非常快。
    *   **瓶颈定位**:
        *   `Both Eye state predictions took: ~720 ms` (每次调用 `_predict_eye_state_tflite` 约 360ms)
        *   `Yawn state prediction took: ~360 ms` (调用 `_predict_yawn_tflite`)
    *   进一步细化到 TFLite `invoke()` 调用耗时：
        *   `_predict_eye_state_tflite: invoke took: ~270 ms`
        *   `_predict_yawn_tflite: invoke took: ~280 ms`
    *   这些 `invoke` 调用占据了各自预测函数的大部分时间，并且是整个 `process_frame` 耗时的主要来源。

**结论**:
*   当前使用的所谓 "INT8" TFLite 模型，由于其输入接口仍为 `float32`，并未实现全整型量化。这意味着在模型推理前，输入数据（来自摄像头的 `uint8` 图像帧）在 `_preprocess_image_for_tflite` 中被转换为 `float32` (除以 255.0)。TFLite 运行时可能需要将这些 `float32` 输入再反量化回 INT8 以匹配内部权重，或者在没有专用硬件加速的情况下，部分运算可能仍在浮点模式下进行。这两种情况都会抵消 INT8 量化本应带来的大部分性能优势。
*   在树莓派这类设备上，对于期望的实时性能，单次模型 `invoke` 耗时 250-300ms 过高。

**后续行动**:
*   **核心任务**: 必须重新审视和修正 TFLite 模型的导出过程。目标是生成 **全整型量化 (full integer quantization)** 的模型，其输入和输出张量类型应为 `int8` 或 `uint8`，并包含正确的量化参数。
*   这将涉及到在模型转换时使用合适的工具和参数，特别是要提供一个有效的**代表性校准数据集 (representative dataset)**。
*   一旦获得真正的全整型量化模型，需要相应调整 `src/ai_logic.py` 中的 `_preprocess_image_for_tflite` 函数，以匹配新的整型输入要求（例如，不再进行 `/255.0` 的操作，并确保 `dtype` 正确）。

## 后续步骤推测
*   分析 `video_stream_generator` 最新的、更详细的日志，找出其未按预期循环和 `yield` 帧的原因。
*   检查是否有任何未捕获的异常或阻塞点在 `video_stream_generator` 的早期逻辑中。
*   确认从 `latest_raw_frame` 获取图像数据是否可靠且持续。
*   ~~一旦视频流问题解决，将进一步测试不同 TFLite 模型的性能和准确性。~~ (此项已可进行)
